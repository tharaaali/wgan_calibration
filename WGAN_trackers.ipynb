{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NfECy3ydw1wk",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.11.0\n",
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "print(\"pytorch version:\", torch.__version__)\n",
    "print(\"GPU\", \"available\" if torch.cuda.is_available() else \"not available\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_frame ( sample, N=15, M=5):\n",
    "\n",
    "    fig, axes = plt.subplots(1,3,  figsize=(N, M))\n",
    "    x = 1  \n",
    "    for i in range(3):\n",
    "            ax = axes[i]\n",
    "            ax.imshow(sample[i], cmap='hot', interpolation='nearest')\n",
    "            ax.axis('off')\n",
    "            mask = (sample[i] == x)\n",
    "                # Get the indices where the mask is True\n",
    "            indices = torch.nonzero(mask)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-TI1qaIw1wr"
   },
   "source": [
    "# Build Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shiftTypes = { \n",
    "            'NoShift': 0,\n",
    "            'moveLeftRight': 1,\n",
    "            'moveUpDown': 2,\n",
    "        }\n",
    "\n",
    "shiftTypesNumber_to_word = {v: k for k, v in shiftTypes.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 num_samples = 1000,\n",
    "                 numberOfSubDetectors = 3,\n",
    "                 subDetector_width = 64,\n",
    "                 subDetector_height = 64,\n",
    "                 numberOfTracksRange=(10,15),\n",
    "                 numberOfNoiseRange=(5,10),\n",
    "                 noisy=True,\n",
    "                 random_seed=42,\n",
    "                 shift_type= None,\n",
    "                 shift_LeftRightRange=(-5,+5),\n",
    "                 shift_UpDownRange=(-5,+5),\n",
    "                 sh_id = None,\n",
    "                 shift_value = None,\n",
    "                 mykeys = None\n",
    "                ):\n",
    "        \n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "            torch.manual_seed(random_seed)\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.numberOfSubDetectors = numberOfSubDetectors\n",
    "        self.subDetector_width= subDetector_width\n",
    "        self.subDetector_height = subDetector_height\n",
    "        self.numberOfTracksRange = numberOfTracksRange\n",
    "        self.numberOfNoiseRange = numberOfNoiseRange\n",
    "        self.noisy= noisy\n",
    "        self.shift_LeftRightRange= shift_LeftRightRange\n",
    "        self.shift_UpDownRange=shift_UpDownRange\n",
    "        self.SubDetectorWithShift=0\n",
    "        self.shift_type=shift_type\n",
    "        self.shift_value= shift_value\n",
    "        self.sh_id = sh_id\n",
    "        self.mykeys = mykeys\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def shift_tensor(self, tensor, axis, n):\n",
    "        shifted_tensor = tensor.clone()\n",
    "        if axis == 0:\n",
    "            if n>0:\n",
    "                shifted_tensor = torch.cat([tensor[n:,:], torch.zeros_like(tensor[:n,:])], dim=0)\n",
    "            elif n<0:\n",
    "                shifted_tensor = torch.cat([torch.zeros_like(tensor[:-n,:]),tensor[:n,:]], dim=0)\n",
    "            else:\n",
    "                raise ValueError(\"shifting value should be integer not 0.\")\n",
    "\n",
    "        elif axis == 1:\n",
    "            if n>0:\n",
    "                shifted_tensor = torch.cat([torch.zeros_like(tensor[:,:n]),tensor[:,:-n]], dim=1)\n",
    "\n",
    "            elif n<0:\n",
    "                shifted_tensor = torch.cat([tensor[:,-n:], torch.zeros_like(tensor[:,:-n])], dim=1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"shifting value should be integer not 0.\")\n",
    "        else:\n",
    "            raise ValueError(\"Axis must be 0 (up down) or 1 (left right).\")\n",
    "        return shifted_tensor\n",
    "    \n",
    "    def shift(self, tensor, shift_type, sh_id):\n",
    "         # 0:noShift, 1:shift along subDetector_width, 2:shift along subDetector_height\n",
    "        axis=None\n",
    "        shift_value=None\n",
    "        shifted_tensor = tensor.clone()\n",
    "        if shift_type == shiftTypes['moveLeftRight']:\n",
    "            axis=1\n",
    "            if self.shift_value is not None:\n",
    "                shift_value = self.shift_value\n",
    "            else:\n",
    "                shift_value = 0\n",
    "                while shift_value == 0:\n",
    "                    shift_value=np.random.randint(self.shift_LeftRightRange[0], self.shift_LeftRightRange[1]+1)\n",
    "            \n",
    "        elif shift_type == shiftTypes['moveUpDown']:\n",
    "            axis=0\n",
    "            if self.shift_value is not None:\n",
    "                shift_value = self.shift_value\n",
    "            else:\n",
    "                shift_value = 0\n",
    "                while shift_value == 0:\n",
    "                    shift_value=np.random.randint(self.shift_UpDownRange[0], self.shift_UpDownRange[1]+1)\n",
    "        \n",
    "        self.SubDetectorWithShift=sh_id\n",
    "        shifted_matrix = self.shift_tensor(shifted_tensor[sh_id], axis, shift_value)\n",
    "        shifted_tensor[sh_id] = shifted_matrix\n",
    "\n",
    "        return shifted_tensor\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        tensor = torch.zeros(self.numberOfSubDetectors, self.subDetector_width, self.subDetector_height,\n",
    "                             dtype=torch.float32)\n",
    "        \n",
    "        # Randomly choose the number of cells for each subdetector\n",
    "        num_of_tracks = np.random.randint(low= self.numberOfTracksRange[0],high=self.numberOfTracksRange[1])\n",
    "        \n",
    "        # Randomly choose num_cells[i] indices to set to 1\n",
    "        indices = np.random.choice(self.subDetector_width * self.subDetector_height, num_of_tracks, replace=False)\n",
    "        row_indices = indices // self.subDetector_height\n",
    "        col_indices = indices % self.subDetector_height\n",
    "            \n",
    "        for i in range(self.numberOfSubDetectors):\n",
    "            tensor[i, row_indices, col_indices] = 1.0\n",
    "        \n",
    "        if self.shift_type is None:\n",
    "            # shift_type = np.random.choice([shiftTypes['NoShift'], shiftTypes['moveLeftRight'], shiftTypes['moveUpDown']])\n",
    "            shift_type = np.random.choice([shiftTypes[x] for x in self.mykeys])\n",
    "        else:\n",
    "            shift_type=self.shift_type\n",
    "        \n",
    "        shiftedTensor=tensor.clone()\n",
    "        shift_values_all_misalignment = torch.zeros(6)\n",
    "        # print('----')\n",
    "        # print(np.where(shiftedTensor.cpu().detach().numpy()[self.sh_id]==1.0))\n",
    "\n",
    "        if shift_type==shiftTypes['NoShift']:\n",
    "            self.SubDetectorWithShift= 0\n",
    "            shift_value = 0\n",
    "        else:\n",
    "            # sh_id=np.random.choice([0,1,2])\n",
    "            if self.sh_id is not None:\n",
    "                sh_id = self.sh_id\n",
    "            shiftedTensor=self.shift(tensor, shift_type, sh_id) \n",
    "            \n",
    "        # print(np.where(shiftedTensor.cpu().detach().numpy()[self.sh_id]==1.0))\n",
    "        if self.noisy==True:\n",
    "            \n",
    "            for subDect_id in range(self.numberOfSubDetectors):\n",
    "                \n",
    "                num_of_noisy_cells= np.random.randint(low= self.numberOfNoiseRange[0],high=self.numberOfNoiseRange[1])   \n",
    "                indices_of_noisy_cells= np.random.choice(self.subDetector_width * self.subDetector_height,\n",
    "                                                         num_of_noisy_cells, replace=False)\n",
    "            \n",
    "                row_indices = indices_of_noisy_cells // self.subDetector_height\n",
    "                col_indices = indices_of_noisy_cells % self.subDetector_height\n",
    "                \n",
    "                for  itr_row, itr_col in zip(row_indices, col_indices):\n",
    "                    ns= torch.rand(1)\n",
    "                    if tensor[subDect_id, itr_row, itr_col] != 1.:\n",
    "                        tensor[subDect_id, itr_row, itr_col] = ns\n",
    "                    if shiftedTensor[subDect_id, itr_row, itr_col] != 1.:\n",
    "                        shiftedTensor[subDect_id, itr_row, itr_col] = ns\n",
    "        \n",
    "        # print(np.where(shiftedTensor.cpu().detach().numpy()[self.sh_id]==1.0))\n",
    "        return {'subDetectors_frames': tensor,\n",
    "                'shiftedSubDetectors_frames':shiftedTensor,\n",
    "                'number_of_tracks': len(indices),\n",
    "                'num_of_tracks':num_of_tracks,\n",
    "                'shift_type': shift_type,\n",
    "                'shift_value' : self.shift_value,\n",
    "                'SubDetectorWithShift':self.SubDetectorWithShift,\n",
    "                # 'shift_values_all_misalignment' :shift_values_all_misalignment\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NoShift': 0, 'moveLeftRight': 1, 'moveUpDown': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shiftTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correct_tensor_global(tensor, axis, n, sh_id):\n",
    "        shifted_tensor = tensor[sh_id].clone()\n",
    "         # Select the channel to use for filling the gaps\n",
    "        channel_cor_from = 0 # let it now be 0 , because we know we only moved channel 1\n",
    "        fill_tensor = tensor[channel_cor_from].clone()\n",
    "        \n",
    "        if axis == 0:\n",
    "            if n>0:\n",
    "                shifted_tensor = torch.cat([tensor[sh_id][n:,:], fill_tensor[:n,:]], dim=0)\n",
    "            elif n<0:\n",
    "                shifted_tensor = torch.cat([fill_tensor[:-n,:],tensor[sh_id][:n,:]], dim=0)\n",
    "            else:\n",
    "                pass\n",
    "                # raise ValueError(\"shifting value should be integer not 0.\")\n",
    "\n",
    "        elif axis == 1:\n",
    "            if n>0:\n",
    "                shifted_tensor = torch.cat([fill_tensor[:,:n],tensor[sh_id][:,:-n]], dim=1)\n",
    "\n",
    "            elif n<0:\n",
    "                shifted_tensor = torch.cat([tensor[sh_id][:,-n:], fill_tensor[:,:-n]], dim=1)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "                # raise ValueError(\"shifting value should be integer not 0.\")\n",
    "        else:\n",
    "            raise ValueError(\"Axis must be 0 (up down) or 1 (left right).\")\n",
    "            \n",
    "        tensor[sh_id]= shifted_tensor \n",
    "        return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpFvaOD35Phk"
   },
   "source": [
    "### Gan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "J4DfoI0y5M1x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, numChannels):\n",
    "        super().__init__()\n",
    "        self.mean_shift_value = None\n",
    "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=16,\n",
    "                               kernel_size=(3, 3), padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32,\n",
    "#        self.conv2 = nn.Conv2d(in_channels=16, out_channels=64,\n",
    "\n",
    "                               kernel_size=(3, 3))\n",
    "#        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        ##\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64,\n",
    "                               kernel_size=(3, 3))\n",
    "        self.batch_norm3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        ##\n",
    "        ##\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                               kernel_size=(2, 2))\n",
    "        self.batch_norm4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.LeakyReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "       \n",
    "        # the next lines commented beacuse the size is smaller 24*24, instead of 3200 to 576\n",
    "        # self.fc1_1 = nn.Linear(in_features=3200, out_features=256)\n",
    "        self.fc1_1 = nn.Linear(in_features=576, out_features=256)\n",
    "        \n",
    "        self.relu1_1 = nn.LeakyReLU()\n",
    "        self.fc1_2 = nn.Linear(in_features=256, out_features=1)\n",
    "\n",
    "        \n",
    "#         self.fc_classif_1 = nn.Linear(in_features=3200, out_features=256)\n",
    "#         self.relu_classif_1 = nn.LeakyReLU()\n",
    "\n",
    "#         self.fc_classif_2 = nn.Linear(in_features=256, out_features=2)\n",
    "\n",
    "        for tensor in [self.conv1, self.conv2, self.conv3, self.conv4, self.fc1_1,\n",
    "                       self.fc1_2]:\n",
    "            torch.nn.init.kaiming_uniform_(tensor.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        corrected_tensor = x.clone()\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        # the next lines commented beacuse the size is smaller 24*24\n",
    "#         x = self.maxpool3(x)\n",
    "\n",
    "#         x = self.conv4(x)\n",
    "#         x = self.relu4(x)\n",
    "#         x = self.batch_norm4(x)\n",
    "        #not this one\n",
    "        # x = nn.AvgPool2d(kernel_size=3, stride=2)(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        output1_1 = self.fc1_1(x)\n",
    "        output1_1 = self.relu1_1(output1_1)\n",
    "        output1_2 = self.fc1_2(output1_1)\n",
    "        \n",
    "        \n",
    "#         output_classif_1 = self.fc_classif_1(x)\n",
    "#         output_classif_1 = self.relu_classif_1(output_classif_1)\n",
    "#         output_classif_2 = self.fc_classif_2(output_classif_1)\n",
    "#         return  output_classif_2\n",
    "\n",
    "        # Convert the output to an integer shift value\n",
    "        # print(output1_2.shape)\n",
    "    \n",
    "        # Apply the shift for a specific channel (e.g., channel 0)\n",
    "        sh_id = 1  # Index of the channel to shift / now we know it's the 1 \n",
    "        shift_type = shiftTypes['moveUpDown']  # now we know it's moveUpDown \n",
    "        self.mean_shift_value = round(output1_2.mean().item()) #let's take the mean now\n",
    "\n",
    "        # print('shift_value',self.mean_shift_value)\n",
    "        \n",
    "        # Apply shift to each sample in the batch\n",
    "        for i in range(corrected_tensor.size(0)):\n",
    "            # shift_value = output1_2[i].item()\n",
    "            # print('shift_value', shift_value)\n",
    "            shifted_matrix = correct_tensor_global(corrected_tensor[i], axis=0, n=-self.mean_shift_value,sh_id=sh_id) #-sign beacuse the correction in the oppisite way \n",
    "            corrected_tensor[i] = shifted_matrix\n",
    "\n",
    "        # return  output1_2\n",
    "        # print(corrected_tensor.shape)\n",
    "        \n",
    "        \n",
    "        # Return the final corrected tensor\n",
    "        return corrected_tensor\n",
    "    \n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim=40, dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_ln_lrelu(in_dim, out_dim, kernel_size=3, padding=1, stride=1):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_dim, out_dim, kernel_size, padding=padding, stride=stride),\n",
    "                nn.InstanceNorm2d(out_dim, affine=True),\n",
    "                nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.ls = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, dim, kernel_size=3, padding=1), nn.LeakyReLU(0.2),\n",
    "            conv_ln_lrelu(dim, dim*2, kernel_size=3, padding=1),\n",
    "            conv_ln_lrelu(dim*2, dim * 2),\n",
    "            conv_ln_lrelu(dim * 2, dim),\n",
    "            nn.Conv2d(dim, 1, 4), # [B,1,21,21]\n",
    "            nn.MaxPool2d(21)) # [B,1,1,1]\n",
    "\n",
    "        self.apply(weight_init)\n",
    "        # init weights\n",
    "        # torch.nn.init.kaiming_uniform_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ls(x)\n",
    "        y = y.view(-1)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "g = Generator(3).to(device)\n",
    "d = Discriminator(in_dim=3, dim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 24, 24]             448\n",
      "         LeakyReLU-2           [-1, 16, 24, 24]               0\n",
      "       BatchNorm2d-3           [-1, 16, 24, 24]              32\n",
      "         MaxPool2d-4           [-1, 16, 12, 12]               0\n",
      "            Conv2d-5           [-1, 32, 10, 10]           4,640\n",
      "         LeakyReLU-6           [-1, 32, 10, 10]               0\n",
      "       BatchNorm2d-7           [-1, 32, 10, 10]              64\n",
      "         MaxPool2d-8             [-1, 32, 5, 5]               0\n",
      "            Conv2d-9             [-1, 64, 3, 3]          18,496\n",
      "        LeakyReLU-10             [-1, 64, 3, 3]               0\n",
      "      BatchNorm2d-11             [-1, 64, 3, 3]             128\n",
      "           Linear-12                  [-1, 256]         147,712\n",
      "        LeakyReLU-13                  [-1, 256]               0\n",
      "           Linear-14                    [-1, 1]             257\n",
      "================================================================\n",
      "Total params: 171,777\n",
      "Trainable params: 171,777\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.32\n",
      "Params size (MB): 0.66\n",
      "Estimated Total Size (MB): 0.99\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 24, 24]           1,792\n",
      "         LeakyReLU-2           [-1, 64, 24, 24]               0\n",
      "            Conv2d-3          [-1, 128, 24, 24]          73,856\n",
      "    InstanceNorm2d-4          [-1, 128, 24, 24]             256\n",
      "         LeakyReLU-5          [-1, 128, 24, 24]               0\n",
      "            Conv2d-6          [-1, 128, 24, 24]         147,584\n",
      "    InstanceNorm2d-7          [-1, 128, 24, 24]             256\n",
      "         LeakyReLU-8          [-1, 128, 24, 24]               0\n",
      "            Conv2d-9           [-1, 64, 24, 24]          73,792\n",
      "   InstanceNorm2d-10           [-1, 64, 24, 24]             128\n",
      "        LeakyReLU-11           [-1, 64, 24, 24]               0\n",
      "           Conv2d-12            [-1, 1, 21, 21]           1,025\n",
      "        MaxPool2d-13              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 298,689\n",
      "Trainable params: 298,689\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.78\n",
      "Params size (MB): 1.14\n",
      "Estimated Total Size (MB): 5.93\n",
      "----------------------------------------------------------------\n",
      "Mean shift value: 1\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(g,(3,24,24))\n",
    "summary(d,(3,24,24))\n",
    "print(f\"Mean shift value: {g.mean_shift_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m visual_frame(\u001b[43mbatch\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubDetectors_frames\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2.5\u001b[39m)\n\u001b[1;32m      2\u001b[0m visual_frame(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshiftedSubDetectors_frames\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shift_type tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "SubDetectorWithShift tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "shift_value tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAB4CAYAAADMkygOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC7klEQVR4nO3dzW7bMBCF0bjoLu//qF07i6KbwHAd6+cjqXO2QQLR0lwMhrRyu9/vHwCc71d9AQBXJYABIgIYICKAASICGCAigAEiv5/98PN2c0btQH8eHAH8vN3e+Tsv/5J7OodX76n7eayja1QHDBARwAARAQwQeToD5ljvzJKA8xxdozpggIgABogIYICIAAaICGCAiAAGiAhggIgABoj4IsZB9nqJx96+X9cI1wSFEWpUBwwQEcAAEQEMEBHAABGbcAcZdXNr1OuCs41QCzpggIgABogIYICIGTBvWfELHSuuibHpgAEiAhggIoABImbAvGXF+eiKa2JsOmCAiAAGiAhggIgABog83YRzMJ09eZ725zOdmw4YICKAASICGCDydAZsnsSePE/785nOTQcMEBHAABEBDBARwACRU96GtuJh8RXXxDm+PzsfH/3zs+LzPMOadMAAEQEMEBHAAJFTZsAjzl62WnFNZxtxFnqGEdc44jVtNcOadMAAEQEMEBHAABEBDBAZ9t/Sz3CImm3c07mp0e10wAARAQwQEcAAkWFnwOZJ12SuOA/3ZjsdMEBEAANEBDBAZNgZMH9d7YU1K6+NNW2pUR0wQEQAA0QEMEBEAANEptmEu9pm1D9XWOMofAlkGzX6czpggIgABogIYIDINDPgK8ySYGZq9Od0wAARAQwQEcAAEQEMEJlmE+4RB+fZk+dnf2r0OR0wQEQAA0QEMEBk6hmweRKvMots+Jyf0wEDRAQwQEQAA0SmngGXrjRTXOFF27NdL9vNUKM6YICIAAaICGCAiAAGiNiE479G3LyAFeiAASICGCAigAEiZsBvMheFsc1QozpggIgABogIYICIAAaI2IR7YIa3KMGVrVKjOmCAiAAGiAhggMjuM2D/PQHGpkbHoQMGiAhggIgABogIYIDI7ptwqwzHYVVqdBw6YICIAAaICGCAyO3+4FA2AMfTAQNEBDBARAADRAQwQEQAA0QEMEDkC33fetE9FNJGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x180 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test visual with correction\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAB4CAYAAADMkygOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC3UlEQVR4nO3dwWoiURBAUR1ml///1Fk7i1kMiIjatrdfec42JORRcCled8z5crmcAPi8X/UvAPCtBBggIsAAEQEGiAgwQESAASK/733x53z2jtqO/tx4BfDnfH7l5zz8TWa6r0/P1Dz3tfc8bcAAEQEGiAgwQOTuHTD7euUuiWMz01n2nqcNGCAiwAARAQaICDBARIABIgIMEBFggIgAA0T8IcZO3vUhHhyHmc5yhHnagAEiAgwQEWCAiAADRDyE24mHM/OY6SxHmKcNGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBA5O6H8Vx/YvwRPryCbcx0FvNcmw0YICLAABEBBojcvQN2nzSPmc5inmuzAQNEBBggIsAAEQEGiHzkvyJPfFl84pmeMfH8E8/0qIlnX+FMNmCAiAADRAQYIPKRO+Aj3r1sNfFMz5h4/olnetTEs69wJhswQESAASICDBARYIDIRx7CvWKFl6h5jpnOYp7b2YABIgIMEBFggMhh74DdJ81jprOY53Y2YICIAANEBBggctg7YP65ftfydHL3tjoznWXLPG3AABEBBogIMEBEgAEiyzyE+9YHF5PPaKazmOfzbMAAEQEGiAgwQGSZO+BvuEv6NmY6i3k+zwYMEBFggIgAA0QEGCCyzEO4W/xX1nnMdBbzvM8GDBARYICIAANElr4Ddp80j5nOYp732YABIgIMEBFggMjSd8Al7zfOY6azrDBPGzBARIABIgIMEBFggIgAA0QEGCAiwAARAQaI+EOMFx3xpW62MdNZVpinDRggIsAAEQEGiAgwQMRDuBtW+BQlnmOms0yZpw0YICLAABEBBoi8/Q74+m7mdFrvfma133dvZjqLeR6HDRggIsAAEQEGiAgwQOTtD+GmXI7zn5nOYp7HYQMGiAgwQESAASLny42XsgHYnw0YICLAABEBBogIMEBEgAEiAgwQ+QtWT4Dl3BvuQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x180 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual without shift\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAB4CAYAAADMkygOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC3UlEQVR4nO3dwWoiURBAUR1ml///1Fk7i1kMiIjatrdfec42JORRcCled8z5crmcAPi8X/UvAPCtBBggIsAAEQEGiAgwQESAASK/733x53z2jtqO/tx4BfDnfH7l5zz8TWa6r0/P1Dz3tfc8bcAAEQEGiAgwQOTuHTD7euUuiWMz01n2nqcNGCAiwAARAQaICDBARIABIgIMEBFggIgAA0T8IcZO3vUhHhyHmc5yhHnagAEiAgwQEWCAiAADRDyE24mHM/OY6SxHmKcNGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBA5O6H8Vx/YvwRPryCbcx0FvNcmw0YICLAABEBBojcvQN2nzSPmc5inmuzAQNEBBggIsAAEQEGiHzkvyJPfFl84pmeMfH8E8/0qIlnX+FMNmCAiAADRAQYIPKRO+Aj3r1sNfFMz5h4/olnetTEs69wJhswQESAASICDBARYIDIRx7CvWKFl6h5jpnOYp7b2YABIgIMEBFggMhh74DdJ81jprOY53Y2YICIAANEBBggctg7YP65ftfydHL3tjoznWXLPG3AABEBBogIMEBEgAEiyzyE+9YHF5PPaKazmOfzbMAAEQEGiAgwQGSZO+BvuEv6NmY6i3k+zwYMEBFggIgAA0QEGCCyzEO4W/xX1nnMdBbzvM8GDBARYICIAANElr4Ddp80j5nOYp732YABIgIMEBFggMjSd8Al7zfOY6azrDBPGzBARIABIgIMEBFggIgAA0QEGCAiwAARAQaI+EOMFx3xpW62MdNZVpinDRggIsAAEQEGiAgwQMRDuBtW+BQlnmOms0yZpw0YICLAABEBBoi8/Q74+m7mdFrvfma133dvZjqLeR6HDRggIsAAEQEGiAgwQOTtD+GmXI7zn5nOYp7HYQMGiAgwQESAASLny42XsgHYnw0YICLAABEBBogIMEBEgAEiAgwQ+QtWT4Dl3BvuQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x180 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shift_type tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "SubDetectorWithShift tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "shift_value tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAB4CAYAAADMkygOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAACxklEQVR4nO3dsW6jQBRA0bDaLv//qamdMnLkeMnKcGeGc9qkQMBcPQ3Y3m632xsA5/tTHwDAVQkwQESAASICDBARYICIAANE/j774/u2eUdtAh+327b3f13TOey9pq7nHH66niZggIgAA0QEGCDydA8Yah8PPir/vu3e8oahmYABIgIMEBFggIgAA0Q8hGNoHrixMhMwQESAASICDBCxB3yi7x8qsL/5bz6IwZnOXqMmYICIAANEBBggIsAAEQ/hTuTh0e+V58xD0+s5+xqbgAEiAgwQEWCAyFJ7wF7aH8cK12K2453BCvfFK5mAASICDBARYIDIUnvAV95LGo1rwSPui3smYICIAANEBBggIsAAkaUewnnJm73cKw3n/Z4JGCAiwAARAQaILLUH7Mu7r+EV5/pV12fPsbg3vlij90zAABEBBogIMEBEgAEiQzyEW+Hl7NmOd2Yjnes9xzLS8f4va/QYJmCAiAADRAQYIDLEHvCIezPAF2v0GCZggIgAA0QEGCAiwACRIR7CjW6Fl9BhZbOuURMwQESAASICDBBZfg/YryfA2K68Rk3AABEBBogIMEBk+T3gkfZQr/Ll3fAbI93zZ69REzBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBAZPlfxOCeX10+1vfz+/bmHPMzEzBARIABIgIMELEHfDGz7UfOtqc68rExHhMwQESAASICDBARYICIAANEBBggIsAAEQEGiPggBkPzwQZWZgIGiAgwQESAASICDBB5+hDOrycca7Zv+mI81uixjl6jJmCAiAADRAQYIPJ0D3i2/aTZ9lRHPjbmMNs9ZI3eMwEDRAQYICLAABEBBogIMEBEgAEiAgwQEWCAyHZ78GI0AMczAQNEBBggIsAAEQEGiAgwQESAASKfWFN9vMLSL+QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x180 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test visual with correction\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAB4CAYAAADMkygOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAACt0lEQVR4nO3dwWrqUBRAUfPozP//VMe3ozeIWAlo3Ml1rVmx0OCBzeEmpMsY4wLA5/2rLwDgWwkwQESAASICDBARYICIAANEfp59eF0Wz6idwG2MZevvmuk5bJ2peZ7DX/O0AQNEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiDx9GQ/vdbv7/3vXZfM7dDgoM53Lp+dpAwaICDBARIABIgIMEHET7oPcoJmPmc7l0/O0AQNEBBggIsAAkanOgO8for5cnNGdnZnOxTzXbMAAEQEGiAgwQGSqM+BvPkualZnOxTzXbMAAEQEGiAgwQESAASJT3YTzkPd8zHQu5rlmAwaICDBARIABIlOdAZdnSf477j7MdC7muWYDBogIMEBEgAEiAgwQOcRNuBkezj7b9e7NTOdinvuwAQNEBBggIsAAkUOcAR/xbIbXmOlczHMfNmCAiAADRAQYICLAAJFD3IQ7uhkeQmfNTOdy1nnagAEiAgwQEWCAyPRnwO94C/67zpK2XMsR39p/NGY6l2+epw0YICLAABEBBogs48Hzc/9dl+XvDzmM2xibD6HM9By2ztQ8z+GvedqAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYIDIz7MPb2Osfr4uy64X823uv9/LZf/v2Ez39emZmue+9p6nDRggIsAAEQEGiDw9Az7beVJxpvqK4tqO/H08YqbH+nuvMs81GzBARIABIgIMEBFggIgAA0QEGCAiwAARAQaILOPBg9EA7M8GDBARYICIAANEBBggIsAAEQEGiPwCeWuPNzlLV8wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x180 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual without shift\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAB4CAYAAADMkygOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAACt0lEQVR4nO3dwWrqUBRAUfPozP//VMe3ozeIWAlo3Ml1rVmx0OCBzeEmpMsY4wLA5/2rLwDgWwkwQESAASICDBARYICIAANEfp59eF0Wz6idwG2MZevvmuk5bJ2peZ7DX/O0AQNEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiDx9GQ/vdbv7/3vXZfM7dDgoM53Lp+dpAwaICDBARIABIgIMEHET7oPcoJmPmc7l0/O0AQNEBBggIsAAkanOgO8for5cnNGdnZnOxTzXbMAAEQEGiAgwQGSqM+BvPkualZnOxTzXbMAAEQEGiAgwQESAASJT3YTzkPd8zHQu5rlmAwaICDBARIABIlOdAZdnSf477j7MdC7muWYDBogIMEBEgAEiAgwQOcRNuBkezj7b9e7NTOdinvuwAQNEBBggIsAAkUOcAR/xbIbXmOlczHMfNmCAiAADRAQYICLAAJFD3IQ7uhkeQmfNTOdy1nnagAEiAgwQEWCAyPRnwO94C/67zpK2XMsR39p/NGY6l2+epw0YICLAABEBBogs48Hzc/9dl+XvDzmM2xibD6HM9By2ztQ8z+GvedqAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYIDIz7MPb2Osfr4uy64X823uv9/LZf/v2Ez39emZmue+9p6nDRggIsAAEQEGiDw9Az7beVJxpvqK4tqO/H08YqbH+nuvMs81GzBARIABIgIMEBFggIgAA0QEGCAiwAARAQaILOPBg9EA7M8GDBARYICIAANEBBggIsAAEQEGiPwCeWuPNzlLV8wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x180 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples = 10000\n",
    "numberOfSubDetectors = 3\n",
    "subDetector_width = 24\n",
    "subDetector_height = 24\n",
    "\n",
    "shift_type=shiftTypes['moveUpDown']\n",
    "\n",
    "shift_value=real_shift_value=4\n",
    "# shifting subdetector \n",
    "sh_id = 1\n",
    "\n",
    "dataset = CustomDataset(num_samples, numberOfSubDetectors, subDetector_width, subDetector_height,\n",
    "                        shift_value=shift_value,\n",
    "                        sh_id = sh_id,\n",
    "                        shift_type=shift_type,\n",
    "                        noisy=False\n",
    "                       )\n",
    "\n",
    "batch_size= 64\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "for batch in [next(iter(train_loader)), next(iter(train_loader))]: \n",
    "    print('shift_type', batch['shift_type'])\n",
    "    print('SubDetectorWithShift', batch['SubDetectorWithShift'])\n",
    "    print('shift_value', batch['shift_value']) \n",
    "    visual_frame(batch['shiftedSubDetectors_frames'][2], 5,2.5)\n",
    "    print( 'test visual with correction')\n",
    "    visual_frame(correct_tensor_global(batch['shiftedSubDetectors_frames'][2], 0, -shift_value,sh_id), 5,2.5)\n",
    "    \n",
    "    print( 'The actual without shift')\n",
    "    \n",
    "    visual_frame(batch['subDetectors_frames'][2], 5,2.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "log= False\n",
    "if log: \n",
    "    \n",
    "    experiment = Experiment(\n",
    "      api_key=\"U4nXlyunaf2RKAIp1UtGkzlSL\",\n",
    "      project_name=\"tracks-detectors-wgan\",\n",
    "      workspace=\"saraaali\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "85538e1466de4e699acd989cbc1f5806",
      "72d906b7df6e47dfa853ad5b9f9dc0bb",
      "3908e03c1df84369951190e16fbc575a",
      "d731e9d89956436bbec0a9329b2f3385",
      "2f6d2646f8b748de8cfb435249203fde",
      "eb4681dfed494e2ebe81c71dca9874bf",
      "3e3d890bb47849e386e922ffd25a2cf8",
      "c6d947db275743239749ef3a7cb67847",
      "e296833ba95e4d9d87dd2a52e4285c9d",
      "1af960071d5243a69cb93730564ce4de",
      "f3fff69b2d7d4329bec227500e8c9eeb"
     ]
    },
    "id": "08vmkqS15Li5",
    "outputId": "ffffc21e-2144-4a66-d4e4-a792db11e33c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 0, step 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cece4bb0d24736a6e1d2484748283b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[D loss: ?] [G loss: ?]:   0%|          | 0/785000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     91\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 92\u001b[0m detector_original_fake \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector_damaged\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m d_fake \u001b[38;5;241m=\u001b[39m d(detector_original_fake)\n\u001b[1;32m     94\u001b[0m d_real \u001b[38;5;241m=\u001b[39m d(detector_original)\n",
      "File \u001b[0;32m/opt/software/python/jupyterhub2/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n\u001b[1;32m    137\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu3(x)\n\u001b[0;32m--> 138\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;66;03m# the next lines commented beacuse the size is smaller 24*24\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#         x = self.maxpool3(x)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m#not this one\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;66;03m# x = nn.AvgPool2d(kernel_size=3, stride=2)(x)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/software/python/jupyterhub2/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/software/python/jupyterhub2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/software/python/jupyterhub2/lib/python3.10/site-packages/torch/nn/functional.py:2422\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2419\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m-> 2422\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2423\u001b[0m )\n",
      "File \u001b[0;32m/opt/software/python/jupyterhub2/lib/python3.10/site-packages/torch/backends/__init__.py:31\u001b[0m, in \u001b[0;36mContextProp.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetter \u001b[38;5;241m=\u001b[39m getter\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetter \u001b[38;5;241m=\u001b[39m setter\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, objtype):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetter()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__set__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, val):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "g = Generator(3).to(device)\n",
    "d = Discriminator(in_dim=3, dim=64).to(device)\n",
    "\n",
    "# d_optimizer = torch.optim.Adam(d.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(d.parameters(), lr=0.01, betas=(0.5, 0.999))\n",
    "# g_optimizer = torch.optim.Adam(g.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = torch.optim.Adam(g.parameters(), lr=0.01, betas=(0.5, 0.999))\n",
    "\n",
    "# Create a directory to save models\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "def save_checkpoint(epoch,step_cnt, g, d, g_optimizer, d_optimizer, checkpoint_dir='saved_models'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'step_cnt': step_cnt,\n",
    "        'generator_state_dict': g.state_dict(),\n",
    "        'discriminator_state_dict': d.state_dict(),\n",
    "        'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "        'd_optimizer_state_dict': d_optimizer.state_dict()\n",
    "    }, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth'))\n",
    "\n",
    "def load_checkpoint(g, d, g_optimizer, d_optimizer, checkpoint_path):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        g.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        d.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "        d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "        epoch= checkpoint['epoch']\n",
    "        step_cnt= checkpoint.get('step_cnt',0)\n",
    "        return epoch,step_cnt\n",
    "    else:\n",
    "        return 0,0\n",
    "\n",
    "def extract_epoch_number(checkpoint_filename):\n",
    "    match = re.search(r'checkpoint_epoch_(\\d+)\\.pth', checkpoint_filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return -1\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "checkpoint_dir = 'saved_models'\n",
    "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_')]\n",
    "\n",
    "# Sort checkpoint files by epoch number\n",
    "checkpoint_files.sort(key=extract_epoch_number)\n",
    "\n",
    "latest_checkpoint = checkpoint_files[-1] if checkpoint_files else None\n",
    "\n",
    "start_epoch , step_cnt=0, 0\n",
    "if latest_checkpoint:\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    start_epoch,step_cnt = load_checkpoint(g, d, g_optimizer, d_optimizer, checkpoint_path)\n",
    "\n",
    "print(f\"Resuming from epoch {start_epoch}, step {step_cnt}\")\n",
    "\n",
    "\n",
    "\n",
    "d.train()\n",
    "g.train()\n",
    "n_epochs = 5000\n",
    "\n",
    "len_iteration = len(train_loader)  \n",
    "\n",
    "save_interval = 5\n",
    "\n",
    "with tqdm(total=n_epochs*len_iteration, desc=f\"[D loss: ?] [G loss: ?]\") as pbar:\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "\n",
    "        \n",
    "        for tracks in train_loader:\n",
    "            \n",
    "            \n",
    "            detector_original = tracks['subDetectors_frames'].to(device) \n",
    "            detector_damaged = tracks['shiftedSubDetectors_frames'].to(device) \n",
    "\n",
    "        \n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            detector_original_fake = g(detector_damaged)\n",
    "            d_fake = d(detector_original_fake)\n",
    "            d_real = d(detector_original)\n",
    "            \n",
    "            d_loss = -torch.mean(d_real)+torch.mean(d_fake)\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            \n",
    "            g_optimizer.zero_grad()\n",
    "            detector_original_fake = g(detector_damaged)\n",
    "            d_fake = d(detector_original_fake)\n",
    "            d_real = d(detector_original)\n",
    "            \n",
    "            g_loss = -torch.mean(d_fake)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "\n",
    "            if log==True:\n",
    "                experiment.log_metric('Disc loss', d_loss.item(), epoch=epoch, step = step_cnt)\n",
    "                experiment.log_metric('Gen loss', g_loss.item(), epoch=epoch, step = step_cnt)\n",
    "\n",
    "            step_cnt += 1\n",
    "    \n",
    "    \n",
    "            \n",
    "            pbar.set_description(f\"[D loss: {d_loss.item():.2f}] [G loss: {g_loss.item():.2f}]\")\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if (step_cnt%20):\n",
    "                predicted_shift_value=  g.mean_shift_value \n",
    "        \n",
    "                diff = abs(real_shift_value - predicted_shift_value )\n",
    "                # print('diff',diff)\n",
    "\n",
    "                if log==True:\n",
    "                    experiment.log_metric('absolut_error',diff, epoch=epoch,step = step_cnt)\n",
    "\n",
    "\n",
    "                    # #  R^2 score\n",
    "                    # r2 = r2_score(real_shift_value, predicted_shift_value)\n",
    "                    # experiment.log_metric('R^2', r2, epoch=epoch, step=step_cnt)\n",
    "    \n",
    "\n",
    "                \n",
    "                \n",
    "         # Check if it's time to save the model\n",
    "        # save_checkpoint(epoch, step_cnt, g, d, g_optimizer, d_optimizer)\n",
    "        print('diff',diff, ', predicted_value=',g.mean_shift_value )\n",
    "                \n",
    "\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "if log==True:\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Basic Python Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1af960071d5243a69cb93730564ce4de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f6d2646f8b748de8cfb435249203fde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3908e03c1df84369951190e16fbc575a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6d947db275743239749ef3a7cb67847",
      "max": 1250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e296833ba95e4d9d87dd2a52e4285c9d",
      "value": 7
     }
    },
    "3e3d890bb47849e386e922ffd25a2cf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72d906b7df6e47dfa853ad5b9f9dc0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb4681dfed494e2ebe81c71dca9874bf",
      "placeholder": "",
      "style": "IPY_MODEL_3e3d890bb47849e386e922ffd25a2cf8",
      "value": "[D loss: -0.78] [G loss: -12.86]:   1%"
     }
    },
    "85538e1466de4e699acd989cbc1f5806": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72d906b7df6e47dfa853ad5b9f9dc0bb",
       "IPY_MODEL_3908e03c1df84369951190e16fbc575a",
       "IPY_MODEL_d731e9d89956436bbec0a9329b2f3385"
      ],
      "layout": "IPY_MODEL_2f6d2646f8b748de8cfb435249203fde"
     }
    },
    "c6d947db275743239749ef3a7cb67847": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d731e9d89956436bbec0a9329b2f3385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1af960071d5243a69cb93730564ce4de",
      "placeholder": "",
      "style": "IPY_MODEL_f3fff69b2d7d4329bec227500e8c9eeb",
      "value": " 7/1250 [00:41&lt;2:05:37,  6.06s/it]"
     }
    },
    "e296833ba95e4d9d87dd2a52e4285c9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb4681dfed494e2ebe81c71dca9874bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3fff69b2d7d4329bec227500e8c9eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
